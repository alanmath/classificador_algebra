{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import autograd.numpy as np_   # Thinly-wrapped version of Numpy\n",
    "from autograd import grad\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dados_avc.csv')\n",
    "df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preencher valores ausentes em 'bmi' com a média\n",
    "df['bmi'].replace('N/A', float('nan'), inplace=True)\n",
    "df['bmi'].fillna(df['bmi'].mean(), inplace=True)\n",
    "\n",
    "# Converter colunas categóricas usando one-hot encoding\n",
    "df = pd.get_dummies(df, columns=['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status'], drop_first=True)\n",
    "df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['stroke', 'id','age', 'bmi', 'avg_glucose_level'], axis=1)\n",
    "Y = df['stroke']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## converting the 0 to -1\n",
    "X = X.replace(0, -1)\n",
    "\n",
    "## convering the True to 1 and False to -1\n",
    "Y = Y.replace(0, -1)\n",
    "X = X.replace(True, 1)\n",
    "X = X.replace(False, -1)\n",
    "\n",
    "X = X.astype('float64')\n",
    "Y = Y.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Since the data is assimetrical, it will be used a subset for training\n",
    "## with all the positive examples and the same nunmber os occurrences of negative examples\n",
    "\n",
    "X_train = X.iloc[:500]\n",
    "Y_train = Y.iloc[:500]\n",
    "X_test = X.iloc[500:]\n",
    "Y_test = Y.iloc[500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss( parametros ):\n",
    "    w, b, x, y = parametros\n",
    "    est = w.T @ x + b\n",
    "    mse = np_.mean( (est - y)**2)\n",
    "    return mse\n",
    "\n",
    "g = grad(loss)\n",
    "x = np.array(X_train).T\n",
    "y = np.array(Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.randn(x.shape[0], 1)\n",
    "b = 0.0\n",
    "alpha = 10**-2\n",
    "\n",
    "for n in range(15000):\n",
    "    grad_ = g( (w, b, x, y) )\n",
    "    w -= alpha*grad_[0]\n",
    "    b -= alpha*grad_[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_test)\n",
    "Y_pred[Y_pred > 0] = 1\n",
    "Y_pred[Y_pred <= 0] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(Y_pred, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_to_1_or_minus_1(x):\n",
    "    if x > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_test, y_est):\n",
    "    return np.mean(np.sign(y_test)==np.sign(y_est))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_test, y_est):\n",
    "    return np.mean(np.sign(y_test)==np.sign(y_est))\n",
    "\n",
    "## create tuples with value of w and X column name\n",
    "w_tuples = []\n",
    "w_lin_model = []\n",
    "for i in range(len(w)):\n",
    "    w_tuples.append((w[i][0], X.columns[i]))\n",
    "    w_lin_model.append((model.coef_[i], X.columns[i]))\n",
    "\n",
    "## sort the tuples\n",
    "w_tuples.sort(reverse=True)\n",
    "w_lin_model.sort(reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w_tuples)\n",
    "\n",
    "x = np.array(X_test).T\n",
    "y = np.array(Y_test)\n",
    "\n",
    "y_est = w.T @ x + b\n",
    "print(f\"Acurácia do modelo: {accuracy(y, y_est) * 100:.2f}%\")\n",
    "\n",
    "print(w_lin_model)\n",
    "\n",
    "print(accuracy_score(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avaliar_hipotese_nula(y_test):\n",
    "    return max(np.count_nonzero(y_test == 1), np.count_nonzero(y_test == -1)) / len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(avaliar_hipotese_nula(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(criterion='entropy')\n",
    "\n",
    "# Agora, vamos usar o método .fit() para ajustar os parâmetros da árvore:\n",
    "tree.fit(X_train, Y_train)\n",
    "\n",
    "# Podemos visualizar a árvore de decisão em uma figura!\n",
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure( figsize=(20,20) )\n",
    "a = plot_tree(tree, feature_names=X.columns, fontsize=15, \n",
    "              node_ids=False, impurity=False, filled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = tree.predict(X_test)\n",
    "accuracy = accuracy_score(Y_test, y_pred)\n",
    "print(\"Acurácia do modelo de árvore de decisão:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Obter importância das características\n",
    "feature_importances = tree.feature_importances_\n",
    "\n",
    "# Criar DataFrame com as features e sua importância\n",
    "feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})\n",
    "\n",
    "# Ordenar o DataFrame pela importância em ordem decrescente\n",
    "feature_importance_df = feature_importance_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "# Exibir o DataFrame\n",
    "print(feature_importance_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
